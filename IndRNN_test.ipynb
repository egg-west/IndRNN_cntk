{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cntk as C\n",
    "import numpy as np\n",
    "\n",
    "from IndRNN import IndRNNStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 943 \n",
    "num_intents = 26    \n",
    "\n",
    "emb_dim    = 150\n",
    "hidden_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_reader(path, is_training):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "         query         = C.io.StreamDef(field='S0', shape=vocab_size,  is_sparse=True),\n",
    "         intent        = C.io.StreamDef(field='S1', shape=num_intents, is_sparse=True)\n",
    "     )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "def create_criterion_function(model, labels):\n",
    "    ce   = C.cross_entropy_with_softmax(model, labels)\n",
    "    errs = C.classification_error      (model, labels)\n",
    "    return ce, errs # (model, labels) -> (loss, error metric)\n",
    "\n",
    "def create_model():\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        return C.layers.Sequential([\n",
    "            C.layers.Embedding(emb_dim, name='embed'),\n",
    "            C.layers.Stabilizer(),\n",
    "            C.layers.Fold(IndRNNStep(hidden_dim), go_backwards=False),# IndRNN\n",
    "            C.layers.Dense(num_intents, name='classify')\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(reader, model_func, max_epochs=10):\n",
    "    \n",
    "    model = model_func(x)\n",
    "    \n",
    "    # Instantiate the loss and error function\n",
    "    loss, label_error = create_criterion_function(model, y)\n",
    "\n",
    "    # training config\n",
    "    epoch_size = 18000        # 18000 samples is half the dataset size \n",
    "    minibatch_size = 100\n",
    "    \n",
    "    # LR schedule over epochs \n",
    "    # In CNTK, an epoch is how often we get out of the minibatch loop to\n",
    "    # do other stuff (e.g. checkpointing, adjust learning rate, etc.)\n",
    "    lr_per_sample = [3e-4]*4+[1.5e-4]\n",
    "    lr_per_minibatch = [lr * minibatch_size for lr in lr_per_sample]\n",
    "    lr_schedule = C.learning_parameter_schedule(lr_per_minibatch, epoch_size=epoch_size)\n",
    "    \n",
    "    # Momentum schedule\n",
    "    momentums = C.momentum_schedule(0.9048374180359595, minibatch_size=minibatch_size)\n",
    "    \n",
    "    learner = C.adam(parameters=model.parameters,\n",
    "                     lr=lr_schedule,\n",
    "                     momentum=momentums, # gradient_clipping_threshold_per_sample=15, \n",
    "                    )\n",
    "\n",
    "    # Setup the progress updater\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=max_epochs)\n",
    "\n",
    "    # Instantiate the trainer\n",
    "    trainer = C.Trainer(model, (loss, label_error), learner, progress_printer)\n",
    "\n",
    "    # process minibatches and perform model training\n",
    "    C.logging.log_number_of_parameters(model)\n",
    "\n",
    "    data_map={x: reader.streams.query, y: reader.streams.intent } \n",
    "\n",
    "    t = 0\n",
    "    for epoch in range(max_epochs):         # loop over epochs\n",
    "        epoch_end = (epoch+1) * epoch_size\n",
    "        while t < epoch_end:                # loop over minibatches on the epoch\n",
    "            data = reader.next_minibatch(minibatch_size, input_map= data_map)  # fetch minibatch\n",
    "            # print(data) # to figure out the dynamic axis\n",
    "            trainer.train_minibatch(data)\n",
    "            t += data[y].num_samples      \n",
    "            if t % 6000 == 0:\n",
    "                training_loss = trainer.previous_minibatch_loss_average\n",
    "                error = trainer.previous_minibatch_evaluation_average\n",
    "                print(\"epoch: {}, step: {}, loss: {:.5f}, error {:.5f}\".format(epoch, t, training_loss, error))\n",
    "        trainer.summarize_training_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = C.sequence.input_variable(vocab_size, name='x_input')\n",
    "y = C.input_variable(num_intents, name='y_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 194877 parameters in 7 parameter tensors.\n",
      "Learning rate per minibatch: 0.03\n",
      "epoch: 0, step: 18000, loss: 0.03331, error 0.00000\n",
      "Finished Epoch[1 of 5]: [Training] loss = 0.644701 * 18000, metric = 15.75% * 18000 12.699s (1417.4 samples/s);\n",
      "Finished Epoch[2 of 5]: [Training] loss = 0.175354 * 18006, metric = 4.86% * 18006 12.270s (1467.5 samples/s);\n",
      "Finished Epoch[3 of 5]: [Training] loss = 0.080272 * 17995, metric = 2.38% * 17995 13.042s (1379.8 samples/s);\n",
      "Finished Epoch[4 of 5]: [Training] loss = 0.070479 * 18004, metric = 1.97% * 18004 12.262s (1468.3 samples/s);\n",
      "Learning rate per minibatch: 0.015\n",
      "epoch: 4, step: 78000, loss: 0.03660, error 0.00000\n",
      "Finished Epoch[5 of 5]: [Training] loss = 0.013461 * 17999, metric = 0.39% * 17999 12.373s (1454.7 samples/s);\n"
     ]
    }
   ],
   "source": [
    "def do_train():\n",
    "    global z\n",
    "    z = create_model()\n",
    "    reader = create_reader('atis.train.ctf', is_training=True)\n",
    "    train(reader, z, 5)\n",
    "do_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Result\n",
    "```\n",
    "Training 690477 parameters in 7 parameter tensors.\n",
    "Learning rate per minibatch: 0.020999999999999998\n",
    "Finished Epoch[1 of 5]: [Training] loss = 0.493163 * 18004, metric = 12.06% * 18004 57.973s (310.6 samples/s);\n",
    "Finished Epoch[2 of 5]: [Training] loss = 0.102100 * 17998, metric = 2.68% * 17998 58.436s (308.0 samples/s);\n",
    "Finished Epoch[3 of 5]: [Training] loss = 0.049302 * 18000, metric = 1.36% * 18000 57.371s (313.7 samples/s);\n",
    "Finished Epoch[4 of 5]: [Training] loss = 0.034251 * 18000, metric = 1.00% * 18000 56.663s (317.7 samples/s);\n",
    "Learning rate per minibatch: 0.010499999999999999\n",
    "Finished Epoch[5 of 5]: [Training] loss = 0.005382 * 17998, metric = 0.13% * 17998 56.685s (317.5 samples/s);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
